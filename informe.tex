\documentclass[a4paper,11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[spanish]{babel}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{float}
\usepackage[hidelinks]{hyperref}
\usepackage[a4paper,margin=2cm,includeheadfoot]{geometry}
\usepackage{lmodern}
\usepackage{setspace}
\usepackage{enumitem}
\usepackage{xcolor}
\usepackage{titlesec}
\usepackage{fancyhdr}
\usepackage{siunitx}
\usepackage{todonotes}


% Espaciado y párrafos
\setstretch{1.1}
\setlength{\parindent}{0pt}
\setlength{\parskip}{0.3em}

% Encabezados y pies de página
\pagestyle{fancy}
\fancyhf{} % limpiar cabezales y pies
\renewcommand{\headrulewidth}{0pt}
\renewcommand{\footrulewidth}{0.4pt}

% Formato de títulos
\titlespacing*{\section}{0pt}{0.5em}{0.3em}
\titlespacing*{\subsection}{0pt}{0.4em}{0.2em}

% Configuración de listings para C++/CUDA
\lstset{
  language=C++,
  basicstyle=\ttfamily\small,
  keywordstyle=\color{blue},
  commentstyle=\color{gray},
  stringstyle=\color{red},
  numbers=left,
  numberstyle=\tiny,
  breaklines=true,
  captionpos=b
}

\begin{document}
\begin{titlepage}
  \thispagestyle{empty}     % sin encabezados/pies
  \centering
  \vspace*{2cm}

  {\Huge\bfseries Práctico 3\\[0.5em]
   Acceso a la Memoria de la GPU\par}
  \vspace{1.5cm}
  \vfill                % empuja lo siguiente hacia abajo

  {\Large
   Joaquin Campo\\
   Mateo Daneri\\
   Santiago Rodriguez\par}
  \vspace{1em}
  {\Large Grupo 34\par}

  \vspace{2cm}
  {\large \today\par}
\end{titlepage}

\section{Parte 1}
El objetivo aqui es implementar un kernel CUDA que, partiendo de una matriz de enteros alojada en memoria global, genera su transpuesta sin recurrir a memoria compartida. Para ello se reservan dos espacios distintos en memoria global (entrada y salida) y se configura los bloques de forma bidimensional. En una primera fase, el kernel se ejecuta con bloques de $32\times 32$ para analizar el patrón de accesos a memoria global de cada warp, medir el tiempo de ejecución promedio (promedio de diez corridas con \textit{Nsight Systems}) y examinar los datos de salida. A continuación, se modifica el tamaño de bloque con el objetivo de reducir los accesos no coalesced; se justifica la elección del nuevo tamaño y se compara el rendimiento obtenido con el caso inicial.

Luego se explorará la influencia de la forma del bloque en la coalescencia de accesos, evaluando configuraciones desde \(1\times1024\) hasta \(1024\times1\) y midiendo las eficiencias de lectura y escritura global (\texttt{gld\_eff}, \texttt{gst\_eff}) para identificar la dimensión que minimice las transacciones no coalesced. Continuando con la implementacion de un kernel de transposición basado en memoria compartida: primero sin padding para cuantificar los conflictos de banco y la eficiencia de shared memory, y luego añadiendo un elemento extra por fila (padding) al tile de \(32\times32\) para eliminar dichos conflictos.


\section{Implementación}
\subsection{Kernel Naive}
Para la transposición de la matriz, se implementó un kernel CUDA sencillo ("naive") que opera únicamente sobre memoria global. Cada hilo calcula su posición 2D dentro de la matriz a partir de los índices de bloque e hilo, y realiza la lectura y escritura correspondiente para efectuar la transposición.

Se empleó un bloque de tamaño $32\times32$, tal como lo indicaba el enunciado, de modo que cada warp (32 hilos) procesa exactamente una fila del bloque y maximiza la coalescencia en las lecturas. La parrilla se dimensiona redondeando hacia arriba la división de las dimensiones de la matriz por el tamaño del bloque, asegurando que todos los elementos queden cubiertos sin dejar datos sin procesar.

% \subsection{Protocolo de Medición SIN VER}
% Para medir con precisión el desempeño:
% \begin{enumerate}[noitemsep]
%   \item Se realizó una ejecución de "warm-up" para estabilizar caches y frecuencias de reloj.
%   \item Se ejecutó el kernel 10 veces, registrando la duración de cada run.
%   \item A partir de los 10 valores, se calculó el promedio y la desviación estándar como métrica de robustez.
% \end{enumerate}

\subsection{Compilación y Ejecución}
Para esta fase se empleó el kernel \texttt{transposeNaive(const int *in, int *out, int rows, int cols)
} sobre una matriz de \(1024\times1024\) elementos. El proyecto cuenta con un sistema de compilación automatizado (\texttt{Makefile}) que invoca \texttt{nvcc} con banderas de optimización específicas para la arquitectura de la GPU, y dispone de un script de ejecución que perfila el programa con \textit{Nsight Systems} para recolectar estadísticas de tiempo y analizar los accesos a memoria. Antes de lanzar el kernel se reservaron dos buffers en memoria global (uno para la matriz de entrada y otro para la salida) y se realizaron las copias de host a dispositivo mediante \texttt{cudaMemcpy}. Esta automatización se realizo con el fin de facilitar la experimentación de distintas configuraciones de kernel.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%5


\subsubsection{Transacciones y eficiencia de memoria}
En la ejecución del kernel \texttt{transposeNaive}, se contabilizaron \num{524290} transacciones de carga totales y \num{1048576} transacciones de store totales en memoria global. A continuación se presenta el resto de las métricas de eficiencia y stalls.

\begin{table}[H]
    \centering
    \caption{Tabla de métricas}
    \begin{tabular}{@{}ll@{}}
        \toprule
        \textbf{Métrica}                  & \textbf{Valor}                       \\ \midrule
        GLD per request                   & 16 transacciones/warp               \\
        GST per request                   & 32 transacciones/warp               \\
        Eficiencia de carga               & 100\,\%                             \\
        Eficiencia de store               & 12,5\,\%                            \\
        Data Request stalls               & 59,2\,\% del tiempo emisión         \\
        Memory Throttle stalls            & 32,1\,\% del tiempo emisión                            \\ \bottomrule
    \end{tabular}
\end{table}

\subsubsection*{Análisis de asimetría lecturas/escrituras}
Para analizar la eficiencia de los accesos a memoria global en CUDA se utilizan métricas como gld\_eff y gst\_eff, que indican el porcentaje de accesos coalesced en lecturas y escrituras, respectivamente. Valores cercanos al 100\% reflejan un uso eficiente del ancho de banda, mientras que valores bajos evidencian accesos dispersos y penalizaciones por transacciones múltiples. Complementariamente, Data Request stalls mide el porcentaje de ciclos en los que los warps están detenidos esperando datos desde memoria global, y Memory Throttle stalls refleja la saturación del subsistema de memoria que impide emitir nuevas solicitudes. Ambas métricas tienden a incrementarse cuando los accesos no están coalesced. Por eso, estos cuatro indicadores son clave para justificar prácticamente la asimetría observada entre lectura y escritura en los kernels. En resumen, el kernel presenta accesos completamente coalesced en lectura y severamente no coalesced en escritura, lo que explica las diferencias de eficiencia observadas.
    


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Medición de Tiempos}
Resultados promedio y desviación estándar:
\begin{table}[H]
\centering
\caption{Tiempos de ejecución (10 runs)}
\begin{tabular}{lrr}
\toprule
Métrica & Valor \mu\text{s} \\
\midrule
Promedio &  98.45  \\
Desviación estándar &  0.02  \\
\bottomrule
\end{tabular}
\end{table}

La duración promedio fue de \SI{98.45}{\micro\second}, acompañada de una desviación estándar de sólo \SI{0.00020}{\milli\second} (por debajo del 0,3\,\% del valor medio), demuestra que el procedimiento ofrece resultados altamente precisos y libres de cambios relevantes. Esta consistencia confirma que las mediciones reflejan fielmente el coste real de los accesos a memoria global, sin verse distorsionadas por variaciones del entorno o del sistema de ejecución. Gracias a esta línea base tan estable, cualquier mejora de rendimiento que se obtenga en las fases posteriores podrá atribuirse a las técnicas de optimización aplicadas y no a artefactos de medición. En la sección siguiente exploraremos el uso de memoria compartida para reducir transacciones no coalesced y evaluar el impacto de estas optimizaciones sobre este punto de partida.





\section{Parte 1.2: Comparación de Tamaños de Bloque}

En esta sección presentamos los resultados de rendimiento y eficiencia de memoria para distintos tamaños de bloque, con el objetivo de identificar la configuración que minimiza los accesos no-coalesced y maximiza el throughput.

\subsection{Resultados de Rendimiento y Coalescencia}

\begin{table}[H]
  \centering
  \caption{Eficiencias de coalescencia para distintos tamaños de bloque}
  \small
  \setlength{\tabcolsep}{3pt}
  \begin{tabular}{l*{13}{S}}
    \toprule
    \textbf{Eficiencia} & \textbf{1×1024} & \textbf{2×512} & \textbf{4×256} & \textbf{8×128} & \textbf{16×64} & \textbf{32×32} & \textbf{64×16} & \textbf{128×8} & \textbf{256×4} & \textbf{512×2} & \textbf{1024×1} \\
    \midrule
    gld\_eff (\%) & 12.50 & 25.00 & 50.00 & 100.00 & 100.00 & 100.00 & 100.00 & 100.00 & 100.00 & 100.00 & 100.00 \\
    gst\_eff (\%) & 100.00 & 100.00 & 25.00 & 25.00 & 12.50 & 12.50 & 12.50 & 12.50 & 12.50 & 12.50 & 12.50 \\
    \bottomrule
  \end{tabular}
\end{table}





\subsection{Análisis de Resultados}

Al analizar la tabla de eficiencias, se observa que tanto \texttt{gld\_eff} como \texttt{gst\_eff} toman valores discretos de 12.5\%, 25\%, 50\% y 100\%. Estos resultados no son casualidad ni errores de medición, sino que reflejan directamente cómo funcionan los accesos a memoria global en la arquitectura de la GPU.

\paragraph{Segmentos y eficiencia.}
La memoria global se accede en segmentos alineados de 128 bytes. Si los hilos de un warp acceden a posiciones que caen en distintos segmentos, el hardware realiza varias transacciones y parte de los datos transferidos se desperdicia. Por ejemplo, si solo un dato útil cae en cada segmento, la eficiencia baja a 12.5\%. En cambio, si todos los hilos acceden a posiciones contiguas dentro de un mismo segmento, se puede alcanzar el 100\% de eficiencia.

\paragraph{Diferencia entre lecturas y escrituras.}
En arquitecturas Pascal (GTX 980ti y GTX 1060), los accesos a memoria global se comportan distinto entre operaciones de lectura y escritura. Las lecturas suelen organizarse en transacciones de 32 bytes, optimizadas para accesos contiguos por parte de los hilos de un warp. En cambio, las escrituras se agrupan en líneas de 64 bytes, lo que implica un mayor riesgo de subutilización del ancho de banda cuando los accesos son desalineados o dispersos. Como resultado, patrones de acceso no contiguos pueden generar múltiples transacciones por escritura, duplicando potencialmente el tráfico necesario respecto a las lecturas. Este detalle es muy importante para entender porque utilizar bloques de 16x64 logra una eficiencia de escritura de 12,5\% en contraste a lo esperado (25\%), mientras que utilizar bloques 8x128 si logra la eficiencia de escritura de 25\%.

\paragraph{Eficiencia y stride.}
La eficiencia observada depende del stride entre los accesos de los hilos de un warp. Si acceden a posiciones separadas por 1, 2, 4 u 8 elementos, la cantidad de datos útiles por transacción varía, y eso se traduce en eficiencias de 100\%, 50\%, 25\% o 12.5\%, respectivamente.

\paragraph{Limitaciones del kernel naive.}
El tamaño del bloque determina si se prioriza la eficiencia de lectura o de escritura. Bloques altos y angostos favorecen las lecturas, mientras que los bloques anchos y bajos favorecen las escrituras. Sin embargo, el kernel naïve nunca puede lograr eficiencia perfecta en ambos sentidos a la vez: siempre estará limitado por el acceso menos alineado.

En resumen, los resultados obtenidos coinciden perfectamente con la teoría: la forma del bloque define cuántos hilos caen en un mismo segmento, la granularidad de las transacciones fija los múltiplos de eficiencia.



\section{Parte II: Transposición con Memoria Compartida}
Antes de analizar en detalle la implementación y optimización del kernel con memoria compartida, es relevante comparar su desempeño frente a las versiones naive de las partes anteriores. En la siguiente tabla se resumen los resultados obtenidos:

\begin{table}[H]
\centering
\caption{Comparativa de kernels: naive vs. shared memory (sin padding)}
\begin{tabular}{lcccccc}
\toprule
Kernel & Tiempo (micro s) & gld\_eff (\%) & gst\_eff (\%) & shared\_eff (\%) & Conflictos (avg/warp) \\
\midrule
Naïve (1.2 8x128) & 58.626 & 100 & 25 & -- & -- \\ 
Shared (sin pad) & 74.818 & 100 & 100 & 51.6\% & 15.5 \\
\bottomrule
\end{tabular}
\end{table}

Se observa que el uso de memoria compartida permite lograr escrituras globales completamente coalesced y, en muchos casos, reduce el tiempo de ejecución en comparación con el kernel \textit{naive}. No obstante, la eficiencia de la memoria compartida se ve limitada por los conflictos de banco y el coste adicional de las barreras de sincronización, lo que introduce un \emph{overhead} que puede hacer que la versión \textit{naive} (sin sincronizaciones) presente un rendimiento similar o incluso superior. Por tanto, aunque la optimización con memoria compartida ofrece mejoras en el acceso global, su beneficio depende del equilibrio entre la reducción de transacciones no coalesced y el coste de sincronización interna.


\paragraph{Implementación y problema principal del kernel shared (sin padding).}
El kernel realiza la transposición en dos fases: cada hilo copia primero un elemento de la matriz global a un tile de $32\times32$ en memoria compartida (\texttt{tile[ty * blockX + tx]}), luego se sincronizan los hilos del bloque y finalmente cada hilo escribe el elemento transpuesto desde el tile compartido a la matriz de salida global (\texttt{tile[tx * blockX + ty]}), logrando lecturas y escrituras globales coalesced. Sin embargo, al no incluir padding en el tile, durante la lectura se producen severos conflictos de banco lo que reduce drásticamente la eficiencia de la shared memory y eleva el número de transacciones por solicitud observadas en el perfilado.

\section{Ejercicio 2 – Parte II: Memoria Compartida con Padding}

\subsection{Explicacion}
Para mitigar los conflictos de banco en la memoria compartida observados en la versión anterior, se introdujo padding en el tile de memoria compartida. Específicamente, al dimensionar el tile en la memoria compartida, se utilizó un \textit{stride} de \texttt{blockDim.x + 1} en lugar de \texttt{blockDim.x}. Esto significa que cada fila del tile en la memoria compartida ocupa \texttt{blockDim.x + 1} elementos, aunque solo se utilicen \texttt{blockDim.x}. Este espaciado adicional tiene como objetivo desplazar las direcciones de memoria accedidas por hilos consecutivos de un warp al leer columnas del tile (durante la segunda fase de la transposición, al escribir a memoria global), de forma que caigan en bancos de memoria distintos, reduciendo así los conflictos.

\subsection{Resultados y Comparativa}
Se compararon los tiempos y conflictos antes y después del padding:

\begin{table}[H]
\centering
\caption{Comparativa de rendimiento: Kernels de transpuesta con memoria compartida }
\begin{tabular}{lcc}
\toprule
\textbf{Métrica} & \textbf{Shared (sin padding)} & \textbf{Shared (con padding)} \\
\midrule
Tiempo de Kernel (\SI{}{\micro\second})       & 74.818  & 43.201  \\
\texttt{gld\_eff} (\%)               & 100     & 100     \\
\texttt{gst\_eff} (\%)               & 100     & 100     \\
Conflictos en Shared (avg/warp) & 15.5    & 0       \\
Eficiencia de Shared (\%)      & 51.6    & 100     \\
\bottomrule
\end{tabular}
\label{tab:shared_comparison_padding}
\end{table}


Observando la Tabla~\ref{tab:shared_comparison_padding}, la introducción del padding en la memoria compartida produce una mejora significativa. El tiempo de ejecución del kernel se redujo de \SI{74,818}{\micro\second} a \SI{43,201}{\micro\second}, lo que representa una aceleración de aproximadamente el 42.27\%.
% Aquí comentar sobre gld_eff y gst_eff para el kernel con padding. Deberían seguir siendo 100% idealmente.
Más importante aún, el número promedio de conflictos de banco en la memoria compartida por warp disminuyó drásticamente de 15.5 a 0, y como consecuencia, la eficiencia de la memoria compartida aumentó de 51.6\% a 100\%. Esta reducción en los conflictos y el aumento en la eficiencia de la memoria compartida son los principales responsables de la mejora en el tiempo de ejecución, ya que permiten que los accesos a la memoria compartida sean mucho más eficientes.
Estos resultados confirman que la técnica de padding es efectiva para aliviar los cuellos de botella generados por los conflictos de banco en la memoria compartida para este patrón de acceso de transposición. Ademas, podemos observar que a diferencia del kernel que utiliza memoria compartida sin padding, este kernel si mejora en tiempo total respecto al kernel naive con bloques de 8x128.


\end{document} 