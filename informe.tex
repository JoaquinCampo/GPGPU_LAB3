\documentclass[a4paper,11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[spanish]{babel}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{float}
\usepackage[hidelinks]{hyperref}
\usepackage[a4paper,margin=2cm,includeheadfoot]{geometry}
\usepackage{lmodern}
\usepackage{setspace}
\usepackage{enumitem}
\usepackage{xcolor}
\usepackage{titlesec}
\usepackage{fancyhdr}
\usepackage{siunitx}

% Espaciado y párrafos
\setstretch{1.1}
\setlength{\parindent}{0pt}
\setlength{\parskip}{0.3em}

% Encabezados y pies de página
\pagestyle{fancy}
\fancyhf{} % limpiar cabezales y pies
\renewcommand{\headrulewidth}{0pt}
\renewcommand{\footrulewidth}{0.4pt}

% Formato de títulos
\titlespacing*{\section}{0pt}{0.5em}{0.3em}
\titlespacing*{\subsection}{0pt}{0.4em}{0.2em}

% Configuración de listings para C++/CUDA
\lstset{
  language=C++,
  basicstyle=\ttfamily\small,
  keywordstyle=\color{blue},
  commentstyle=\color{gray},
  stringstyle=\color{red},
  numbers=left,
  numberstyle=\tiny,
  breaklines=true,
  captionpos=b
}

\begin{document}
\begin{titlepage}
  \thispagestyle{empty}     % sin encabezados/pies
  \centering
  \vspace*{2cm}

  {\Huge\bfseries Práctico 3\\[0.5em]
   Acceso a la Memoria de la GPU\par}
  \vspace{1.5cm}
  \vfill                % empuja lo siguiente hacia abajo

  {\Large
   Joaquin Campo\\
   Mateo Daneri\\
   Santiago Rodriguez\par}
  \vspace{1em}
  {\Large Grupo 34\par}

  \vspace{2cm}
  {\large \today\par}
\end{titlepage}

\section{Introducción}
En este informe presentamos directamente la implementación, el protocolo de medición y los resultados obtenidos en la Primera Parte del Ejercicio 1, sin extenderse en teoría general.

\section{Plan de Trabajo}
El objetivo aqui es implementar un kernel CUDA que, partiendo de una matriz de enteros alojada en memoria global, genera su transpuesta sin recurrir a memoria compartida. Para ello se reservan dos espacios distintos en memoria global (entrada y salida) y se configura los bloques de forma bidimensional. En una primera fase, el kernel se ejecuta con bloques de $32\times 32$ para analizar el patrón de accesos a memoria global de cada warp, medir el tiempo de ejecución promedio (promedio de diez corridas con \textit{Nsight Systems}) y examinar los datos de salida. A continuación, se modifica el tamaño de bloque con el objetivo de reducir los accesos no coalesced; se justifica la elección del nuevo tamaño y se compara el rendimiento obtenido con el caso inicial.

\section{Implementación}
\subsection{Kernel Naïve}
Para la transposición de la matriz, se implementó un kernel CUDA sencillo ("naïve") que opera únicamente sobre memoria global. Cada hilo calcula su posición 2D dentro de la matriz a partir de los índices de bloque e hilo, y realiza la lectura y escritura correspondiente para efectuar la transposición.

Se eligieron bloques de tamaño $32\times32$ porque cada warp (32 hilos) cubre exactamente una fila de ese bloque, lo que maximiza la posibilidad de accesos coalesced en un primer acercamiento. El grid se dimensiona con divisiones redondeadas hacia arriba para cubrir toda la matriz, garantizando que todos los elementos se procesen en la rutina.

% \subsection{Protocolo de Medición SIN VER}
% Para medir con precisión el desempeño:
% \begin{enumerate}[noitemsep]
%   \item Se realizó una ejecución de "warm-up" para estabilizar caches y frecuencias de reloj.
%   \item Se ejecutó el kernel 10 veces, registrando la duración de cada run.
%   \item A partir de los 10 valores, se calculó el promedio y la desviación estándar como métrica de robustez.
% \end{enumerate}

\subsection{Compilación y Ejecución}
Para esta fase se empleó el kernel \texttt{transposeNaive(int* out, const int* in, int W, int H)} sobre una matriz de \(1024\times1024\) elementos, definiendo cada bloque con un tamaño de \([32,32]\) de modo que cada warp procesa exactamente una fila o columna de 32 valores en una única invocación. El proyecto cuenta con un sistema de compilación automatizado (\texttt{Makefile}) que invoca \texttt{nvcc} con banderas de optimización específicas para la arquitectura de la GPU, y dispone de un script de ejecución que perfila el programa con \textit{Nsight Systems} para recolectar estadísticas de tiempo y analizar los accesos a memoria. Antes de lanzar el kernel se reservaron dos buffers en memoria global (uno para la matriz de entrada y otro para la salida) y se realizaron las copias de host a dispositivo mediante \texttt{cudaMemcpy}. Esta automatización no solo asegura reproducibilidad, sino que también facilita la experimentación con distintos tamaños de matriz y configuraciones de kernel.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%5




\subsection{Kernel Naïve: Análisis con bloque $32\times32$}

\subsubsection{Configuración y medición de tiempo}
Se lanzó el kernel \texttt{transposeNaive} con
\[
  \text{blockDim} = (32,32),\quad
  \text{gridDim}  = (\,\lceil\mathrm{cols}/32\rceil,\;\lceil\mathrm{rows}/32\rceil)
  = (32,32)
\]
y se midió el tiempo de ejecución usando eventos CUDA:
\[
  \text{Duración} = 98.206\;\mu\text{s}
\]

\subsubsection{Patrón de acceso a memoria global por warp}
\begin{itemize}
  \item \textbf{Lecturas (loads):}  
    Cada warp (32 hilos con igual \texttt{threadIdx.y}) lee  
    \(\texttt{in}[\,\text{row}*\,\mathrm{cols} + \text{col}+i]\) para \(i=0\ldots31\),  
    es decir, 32 palabras consecutivas y alineadas en memoria.  
    \begin{itemize}
      \item \emph{Global Memory Load Efficiency:} 100 \%  
      \item \emph{GLD Transactions per Request:} 16 transacciones de memoria por warp  
    \end{itemize}
    → \emph{lecturas coalesced}.
  \item \textbf{Escrituras (stores):}  
    Cada warp escribe  
    \(\texttt{out}[\,(\text{col}+i)*\,\mathrm{rows} + \text{row}]\) para \(i=0\ldots31\).  
    El stride de \(\mathrm{rows}\) entre hilos provoca que cada write lance  
    su propia transacción de 4 B en lugar de agruparse.  
    \begin{itemize}
      \item \emph{Global Memory Store Efficiency:} 12.5 \%  
      \item \emph{GST Transactions per Request:} 32 transacciones de memoria por warp  
    \end{itemize}
    → \emph{writes no coalesced}.
\end{itemize}

\subsubsection{Impacto en rendimiento}
\begin{itemize}
  \item \textbf{Bursts de stalls por memoria:}  
    - \emph{Data Request stalls:} 59.2 \% del tiempo de emisión.  
    - \emph{Memory Throttle stalls:} 32.1 \%.  
  \item \textbf{Ocupación y paralelismo:}  
    - Achieved Occupancy = 0.673 (no limitado por registros ni shared).  
    - Warp Execution Efficiency = 100 \% (sin divergencia de control).  
\end{itemize}

\subsubsection{Analisis de los resultadose}
El estudio muestra una marcada asimetría entre lecturas y escrituras: mientras las cargas están completamente coalesced (\textbf{16 transacciones por warp}), las escrituras no lo están, generando \textbf{32 transacciones por warp} y solo un \textbf{12{,}5\,\%} de eficiencia de store. Esta ineficiencia en las escrituras provoca el \textbf{59{,}2\,\%} de ciclos de \emph{Data Request stalls} y el \textbf{32{,}1\,\%} de \emph{Memory Throttle stalls}. A pesar de contar con una ocupación de \(0{,}673\) y una Warp Execution Efficiency del \textbf{100\,\%}, el rendimiento queda acotado por el elevado número de transacciones de escritura no coalesced, reduciendo drásticamente el ancho de banda efectivo. Para atenuar la penalización debida a las escrituras no coalesced, se puede experimentar con bloques rectangulares que reduzcan el stride entre direcciones de store.

    


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Medición de Tiempos}
Resultados promedio y desviación estándar para bloque 32\,$\times$\,32 y tamaño de matriz 1024×1024:
\begin{table}[H]
\centering
\caption{Tiempos de ejecución (10 runs) para 1024×1024}
\begin{tabular}{lrr}
\toprule
Métrica & Valor \mu\text{s} \\
\midrule
Promedio &  98.45  \\
Desviación estándar &  0.02  \\
\bottomrule
\end{tabular}
\end{table}

La duración promedio fue de \SI{98.45}{\micro\second}, acompañada de una desviación estándar de sólo \SI{0.00020}{\milli\second} (por debajo del 0,3\,\% del valor medio), demuestra que el procedimiento ofrece resultados altamente precisos y libres de cambios relevantes. Esta consistencia confirma que las mediciones reflejan fielmente el coste real de los accesos a memoria global, sin verse distorsionadas por variaciones del entorno o del sistema de ejecución. Gracias a esta línea base tan estable, cualquier mejora de rendimiento que se obtenga en las fases posteriores podrá atribuirse a las técnicas de optimización aplicadas y no a artefactos de medición. En la sección siguiente exploraremos el uso de memoria compartida para reducir transacciones no coalesced y evaluar el impacto de estas optimizaciones sobre este punto de partida.


\section{Análisis de Accesos}
Para entender el patrón de memoria global, se perfiló el kernel con Nsight Systems y se inspeccionaron las transacciones de memoria por warp:
\begin{itemize}[noitemsep]
  \item \textbf{Lectura:} cada warp generó alrededor de X transacciones de 32 B al leer una fila de 32 elementos, mostrando accesos coalesced segmentados.
  \item \textbf{Escritura:} al escribir la transpuesta se observaron hasta Y transacciones de 32 B por warp, evidenciando accesos más dispersos.
\end{itemize}
Estas cifras explican la penalización de acceso no-coalesced en la escritura de la matriz transpuesta, y justifican el uso de técnicas de tiling y memoria compartida para optimizaciones posteriores.


\section{Parte 1.2: Comparación de Tamaños de Bloque}

En esta sección presentamos los resultados de rendimiento y eficiencia de memoria para distintos tamaños de bloque, con el objetivo de identificar la configuración que minimiza los accesos no-coalesced y maximiza el throughput.

\subsection{Resultados de Rendimiento y Coalescencia}

\begin{table}[H]
  \centering
  \caption{Cuadro 2: Eficiencias de coalescencia para distintos tamaños de bloque}
  \small
  \setlength{\tabcolsep}{1.5pt}
  \begin{tabular}{l*{13}{S}}
    \toprule
    \textbf{Eficiencia} & \textbf{1×1024} & \textbf{2×512} & \textbf{4×256} & \textbf{8×128} & \textbf{16×64} & \textbf{32×32} & \textbf{64×16} & \textbf{128×8} & \textbf{256×4} & \textbf{512×2} & \textbf{1024×1} \\
    \midrule
    gld\_eff (\%) & 12.50 & 25.00 & 50.00 & 100.00 & 100.00 & 100.00 & 100.00 & 100.00 & 100.00 & 100.00 & 100.00 \\
    gst\_eff (\%) & 100.00 & 100.00 & 25.00 & 25.00 & 12.50 & 12.50 & 12.50 & 12.50 & 12.50 & 12.50 & 12.50 \\
    \bottomrule
  \end{tabular}
\end{table}


\subsection{Análisis de Resultados}

Al analizar la tabla de eficiencias, se observa que tanto \texttt{gld\_eff} como \texttt{gst\_eff} toman valores discretos de 12.5\%, 25\%, 50\% y 100\%. Estos resultados no son casualidad ni errores de medición, sino que reflejan directamente cómo funcionan los accesos a memoria global en la arquitectura de la GPU.

\paragraph{Segmentos y eficiencia.}
En GPUs modernas (a partir de compute capability 3.x), la memoria global se accede en segmentos alineados de 128 bytes. Si los hilos de un warp acceden a posiciones que caen en distintos segmentos, el hardware realiza varias transacciones y parte de los datos transferidos se desperdicia. Por ejemplo, si solo un dato útil cae en cada segmento, la eficiencia baja a 12.5\%. En cambio, si todos los hilos acceden a posiciones contiguas dentro de un mismo segmento, se puede alcanzar el 100\% de eficiencia.

\paragraph{Diferencia entre lecturas y escrituras.}
Otra cuestión importante es que las lecturas y escrituras no siempre se comportan igual. En arquitecturas como Pascal, las lecturas suelen agruparse en transacciones de 32 bytes, mientras que las escrituras se agrupan en líneas de 64 bytes. Por eso, cuando los hilos no son contiguos, las escrituras pueden ser el doble de costosas en términos de ancho de banda desperdiciado, lo que explica por qué a veces la eficiencia de lectura sube mientras la de escritura baja, y viceversa, según la forma del bloque.

\paragraph{Eficiencia y stride.}
La eficiencia observada depende del "stride" entre los accesos de los hilos de un warp. Si acceden a posiciones separadas por 1, 2, 4 u 8 elementos, la cantidad de datos útiles por transacción varía, y eso se traduce en eficiencias de 100\%, 50\%, 25\% o 12.5\%, respectivamente.

\paragraph{Limitaciones del kernel naïve.}
El tamaño del bloque determina si se prioriza la eficiencia de lectura o de escritura. Bloques altos y angostos favorecen las lecturas, mientras que los bloques anchos y bajos favorecen las escrituras. Sin embargo, el kernel naïve nunca puede lograr eficiencia perfecta en ambos sentidos a la vez: siempre estará limitado por el acceso menos alineado.

\paragraph{Memoria compartida y tiling.}
La solución para superar esta limitación es usar memoria compartida y técnicas de tiling. Al copiar un tile a shared memory, transponerlo localmente y luego escribirlo de vuelta de forma coalesced, se pueden lograr eficiencias cercanas al 100\% tanto en lecturas como en escrituras. Además, agregar padding en la memoria compartida ayuda a evitar conflictos de banco y aprovechar al máximo el ancho de banda.

En resumen, los resultados obtenidos coinciden perfectamente con la teoría: la forma del bloque define cuántos hilos caen en un mismo segmento, la granularidad de las transacciones fija los múltiplos de eficiencia, y solo el uso de memoria compartida permite optimizar ambos sentidos de acceso a la vez.

\section{Parte II: Transposición con Memoria Compartida}
Antes de analizar en detalle la implementación y optimización del kernel con memoria compartida, es relevante comparar su desempeño frente a las versiones naïve de las partes anteriores. En la siguiente tabla se resumen los resultados obtenidos para bloque $32\times32$ y matriz $1024\times1024$:

\begin{table}[H]
\centering
\caption{Comparativa de kernels: naïve vs. shared memory (sin padding), bloque $32\times32$}
\begin{tabular}{lcccccc}
\toprule
Kernel & Tiempo (ms) & gld\_eff (\%) & gst\_eff (\%) & shared\_eff (\%) & Conflictos (avg/warp) \\
\midrule
Naïve (1.1/1.2) & 0.098 & 100 & 12.5 & -- & -- \\
Shared (sin pad) & 0.074 & 100 & 100 & 6.06 & 31 \\
\bottomrule
\end{tabular}
\end{table}

Se observa que el uso de memoria compartida permite lograr escrituras globales coalesced y reduce el tiempo de ejecución respecto al kernel naïve. Sin embargo, la eficiencia de la memoria compartida es baja debido a conflictos de banco, lo que limita la mejora alcanzada.


\paragraph{Implementación resumida del kernel shared (sin padding).}
El kernel implementa la transposición en dos fases: primero, cada hilo copia un elemento de la matriz global a un tile en memoria compartida de tamaño $32\times32$ (\texttt{tile[ty * blockX + tx]}). Tras sincronizar los hilos del bloque, cada hilo escribe el elemento transpuesto desde el tile compartido a la matriz de salida global (\texttt{tile[tx * blockX + ty]}). Esta estrategia permite que tanto las lecturas como las escrituras globales sean coalesced, pero no utiliza padding en el tile compartido.

\textbf{Problema principal:} Al no emplear padding, los accesos a memoria compartida durante la lectura del tile generan severos conflictos de banco, ya que múltiples hilos de un warp acceden a direcciones que caen en el mismo banco de memoria compartida. Esto se refleja en la baja eficiencia de shared memory y en el alto número de transacciones por request observados en el perfilado.

\section{Ejercicio 2 – Parte II: Memoria Compartida con Padding}

\subsection{Explicacion}
Para mitigar los conflictos de banco en la memoria compartida observados en la versión anterior, se introdujo padding en el tile de memoria compartida. Específicamente, al dimensionar el tile en la memoria compartida, se utilizó un \textit{stride} de \texttt{blockDim.x + 1} en lugar de \texttt{blockDim.x}. Esto significa que cada fila del tile en la memoria compartida ocupa \texttt{blockDim.x + 1} elementos, aunque solo se utilicen \texttt{blockDim.x}. Este espaciado adicional tiene como objetivo desplazar las direcciones de memoria accedidas por hilos consecutivos de un warp al leer columnas del tile (durante la segunda fase de la transposición, al escribir a memoria global), de forma que caigan en bancos de memoria distintos, reduciendo así los conflictos.

\subsection{Resultados y Comparativa}
Se compararon los tiempos y conflictos antes y después del padding:

\begin{table}[H]
\centering
\caption{Comparativa de rendimiento: Kernels de transposición con memoria compartida (matriz $1024 \times 1024$, bloque $32 \times 32$)}
\begin{tabular}{lSS}
\toprule
Métrica & {Shared (sin padding)} & {Shared (con padding)} \\
\midrule
Tiempo de Kernel (ms)       & 0.074  & 0.043  \\
gld\_eff (\%)               & 100    & {X.XX}   \\ % From nvprof
gst\_eff (\%)               & 100    & {X.XX}   \\ % From nvprof
Shared Conflicts (avg/warp) & 31     & {Y.YY}   \\ % From nvprof --metrics all
Shared Efficiency (\%)      & 6.06   & {Z.ZZ}   \\ % From nvprof --metrics all
\bottomrule
\end{tabular}
\label{tab:shared_comparison_padding}
\end{table}

Observando la Tabla~\ref{tab:shared_comparison_padding}, la introducción del padding en la memoria compartida produce una mejora significativa. El tiempo de ejecución del kernel se redujo de 0.074\,ms a 0.043\,ms, lo que representa una aceleración de aproximadamente el XX\%.
% Aquí comentar sobre gld_eff y gst_eff para el kernel con padding. Deberían seguir siendo 100% idealmente.
Más importante aún, el número promedio de conflictos de banco en la memoria compartida por warp disminuyó drásticamente de 31 a Y.YY, y como consecuencia, la eficiencia de la memoria compartida aumentó de 6.06\% a Z.ZZ\%. Esta reducción en los conflictos y el aumento en la eficiencia de la memoria compartida son los principales responsables de la mejora en el tiempo de ejecución, ya que permiten que los accesos a la memoria compartida sean mucho más eficientes.
Estos resultados confirman que la técnica de padding es efectiva para aliviar los cuellos de botella generados por los conflictos de banco en la memoria compartida para este patrón de acceso de transposición.

\section{Conclusion}

\end{document} 